{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zePWZmxY2vOb"
   },
   "source": [
    "# Standardize and Correct Data Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTtj0-Rx297R"
   },
   "source": [
    "First, let's import some useful libraries. These are ready-made sets of commands which can do tasks; in our case, we want tasks such as reading CSV files, merging them, summarizing them, etc. We will mainly use **pandas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xbyd9CI0YLX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['State'] = df1['State'].str.lstrip(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning\\Sheets_to_Clean\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning\\/Sheets_to_Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will edit the following 67 files\n",
      "['Albuquerque_trees.csv', 'Anaheim_trees.csv', 'Arlington_trees.csv', 'Atlanta_trees.csv', 'AuroraCO_trees.csv', 'Austin_trees.csv', 'Baltimore_trees.csv', 'Boston_trees.csv', 'Buffalo_trees.csv', 'CapeCoral_trees.csv', 'ColoradoSprings_trees.csv', 'Columbus_trees.csv', 'Dallas_trees.csv', 'Denver_trees.csv', 'DesMoines_trees.csv', 'Detroit_tree.csv', 'Durham_trees.csv', 'ElPasoPERCENT_trees.csv', 'Fresno_trees.csv', 'GardenGrove_trees.csv', 'GrandRapids_trees.csv', 'Greensboro_trees.csv', 'Honolulu_trees.csv', 'Houston_trees.csv', 'HuntingtonBeach_trees.csv', 'Indianapolis_trees.csv', 'Irvine_trees.csv', 'Jerseycity_trees.csv', 'Knoxville_trees.csv', 'LasVegas_trees.csv', 'LosAngeles1_trees.csv', 'LosAngeles2_trees.csv', 'LosAngeles3_trees.csv', 'LosAngeles4_trees.csv', 'Louisville_trees.csv', 'Madison_trees.csv', 'Miami_trees.csv', 'Milwaukee_trees.csv', 'Minneapolis_trees.csv', 'Nashville_trees.csv', 'NewOrleans_trees.csv', 'NewYork_trees.csv', 'Oakland_trees.csv', 'OklahomaCity_trees.csv', 'Ontario_trees.csv', 'Orlando_trees.csv', 'OverlandPark_trees.csv', 'Phoenix_trees.csv', 'Pittsburgh_trees.csv', 'Plano_trees.csv', 'Portland_trees.csv', 'Providence_trees.csv', 'RanchoCucamonga_trees.csv', 'Richmond_trees.csv', 'Rochester_trees.csv', 'Sacramento_trees.csv', 'SanDiego_trees.csv', 'SanFrancisco_trees.csv', 'SanJose_trees.csv', 'SantaRosa_trees.csv', 'Seattle_trees.csv', 'SiouxFalls_trees.csv', 'StLouis_trees.csv', 'Stockton_trees.csv', 'Tampa_trees.csv', 'WashingtonDC_trees.csv', 'Worcester_trees.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob('*.csv')\n",
    "print(\"I will edit the following \" + str(len(csv_files))+ \" files\")\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edit all important columns\n",
    "\n",
    "#### scientific names\n",
    "- strip leading space (df1['State'] = df1['State'].str.strip())\n",
    "- replace ' X ' with ' ' df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n",
    "- replace ' x ' with ' '\n",
    "- remove anyything in quotes 'darlingtoni'\n",
    "- Take first two words, make capitalization , Genus species \n",
    "- replace with \"\" the wide variety of non-tree answers\n",
    "- replace a few very common name typos, caught manually\n",
    "#### common names:\n",
    "- fix cases wher it is split by a comma\n",
    "- remove non ascii characters\n",
    "- fix typos\n",
    "- replace spp etc with ''\n",
    "- capitalize first word\n",
    "#### Address Issues       \n",
    "- fix capitalizations for addresses\n",
    "- combine street name and street number\n",
    "- if no street number, just keep street name as the address\n",
    "#### Overhead Utility       \n",
    "- categorize as yes or no based on the variety of ways cities code it\n",
    "#### Columns that just need simple capitalization fixes, etc\n",
    "- 'location_type','location_name','neighborhood','district','planter_type','ward'\n",
    "- remove non ASCII characters\n",
    "- strip spaces\n",
    "- capitalize\n",
    "#### Height\n",
    "- convert FT columns to meters\n",
    "#### Height Range\n",
    "- standardize the height ranges presented to us\n",
    "#### Bin Heights\n",
    "- for analyses where we bin heights, to include as many cities as possible\n",
    "- sort into bins\n",
    "#### Diameter Breast Height\n",
    "- convert IN to CM\n",
    "#### Diameter Breast Height Range\n",
    "- standardize \n",
    "#### Diameter Breast Height Binned\n",
    "- for analyses where we bin diameters, to include as many cities as possible\n",
    "- sort into bins\n",
    "#### Condition\n",
    "- standardize the many ways cities code condition\n",
    "- note that we checked with specific city metadata where needed\n",
    "#### Date Columns\n",
    "- cities recorded a wide variety of dates\n",
    "- cycle through all date like columsn adn standardize them\n",
    "- take the most recent observation out of 'inventory_date', 'condition_date','edit_date', 'inspected_date' and save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (15,18,25,32,38,39,40,41,42,45,50,62,63) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (24,25,26,27,28,29,30,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (16,17,18,19,20,24,25,26,27,28,29,30,31,32,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (6,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (18,20,48) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (0,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4,5,9,11,12,17,18,34,37,39,57) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (34,35,36,37,40,45,71,72) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (37,38,71,72) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (71) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4,14,15,17,19,21,40,51,62,71,72,73,75) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (3,31,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (11,12,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (7,9,10,21,39) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (9,35,45) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (26,27,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-5-6627c810a27a>:490: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n"
     ]
    }
   ],
   "source": [
    "# set path for where to save edited files\n",
    "path=r'C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning\\Sheets_Data_Misspellings_Corrected'\n",
    "\n",
    "for file in csv_files:\n",
    "#for file in csv_files[25:26]:\n",
    "#for file in csv_files[26:56]:    \n",
    "    data = pd.read_csv(file)\n",
    "    # save the state name\n",
    "    state = file.split('_')[0]\n",
    "    #if it has scientific names\n",
    "#############################\n",
    "###  Scientific Name      \n",
    "#############################\n",
    "    if 'scientific_name' in data.columns:\n",
    "        data['scientific_name_old']=data['scientific_name']\n",
    "        ## quick fix to capitalization issues\n",
    "        data['scientific_name']=data['scientific_name'].str.capitalize()\n",
    "        ## fix issues\n",
    "        data['scientific_name']=data['scientific_name'].str.strip()\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\" X \", \" \")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\"X \", \"\")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\"Ã—\",\"\")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\"Â\",\"\")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\" x \", \" \")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\" x$\", \"\")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\"::\", \" \")\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(r'\\'.*?\\'', '')   \n",
    "        data['scientific_name'] = data['scientific_name'].str.replace('*', '')  \n",
    "        data['scientific_name'] = data['scientific_name'].str.replace('?', '') \n",
    "        # now get rid of vacant sites, stumps, etc.\n",
    "        #data['scientific_name'] = data['scientific_name'].str.replace(r'Vacant.*','')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(r'Stump.*', '')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace('Stump-', '')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace('Standing butt', '')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(r'Unknown.*', '')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(r'Other.*', '')\n",
    "        # formerly replaced all these with ' sp.'\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\" spp\\.\", '')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\" spp\", '')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(\" species\", '')\n",
    "        # data['scientific_name'] = data['scientific_name'].str.replace(r' sp$','')\n",
    "        data['scientific_name'] = data['scientific_name'].str.replace(' sp\\.','')\n",
    "        ##########\n",
    "        ## remove non ASCII characters\n",
    "        data['scientific_name'] = data['scientific_name'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "        ## get two name format, remove subspecies\n",
    "        names = data['scientific_name'].str.split(expand=True)[[0,1]]\n",
    "        names[1].replace(to_replace=[None], value='', inplace=True)\n",
    "        data['scientific_name'] = names[0].str.lower().str.capitalize() + ' ' + names[1].str.lower()\n",
    "        data['scientific_name'] = data['scientific_name'].str.strip()\n",
    "        ########## REPLACE a bunch of manual typos\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Rock'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Quercus/live virginiana'], 'Quercus virginiana')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Mixed'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Unidentifiable tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Mixed brush'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Hardwood'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['#name'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace([')amelanchier canadensis'], 'Amelanchier canadensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace([')carpinus caroliniana'], 'Carpinus caroliniana')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['0'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['10:15:54'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['15948'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['443'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['444'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['445'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['446'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['448'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['A. arnoldiana\\'autmn'], 'Aesculus arnoldiana')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Acre rubrum'], 'Acer rubrum')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Acre saccharinum'], 'Acer saccharum')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Alder'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Almond tree'], 'Prunus dulcis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Angohpora angophora'], 'Angophora ')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['AngophoraÂ '], 'Angophora')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Apricot tree'], 'Prunus')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Arborvitae sp'], 'Thuja')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Arecacea'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Arecaecea'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Ash'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Banana'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Bonsai'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Broadleaf deciduous'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Broadleaf evergreen'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Calls'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Camaldulensis'], 'Eucalyptus camaldulensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Cedric deodora'], 'Cedrus deodara')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Cherry/prunus'], 'Prunus')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Cherry\\/prunus'], 'Prunus sp.')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Chitalpatashkentensis'], 'Chitalpa tashkentensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Clerics canadensis'], 'Cercis canadensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Conifer evergreen'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Cunnighamia'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Ddigs tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Dead tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Dead'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Empty pit'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['EucalyptusÂ camaldulensis'], 'Eucalyptus camaldulensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Eucommiaulmoides'], 'Eucommia ulmoides')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Fig tree'], 'Ficus sp.')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['For unknown'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Forthysia'], 'Forsythia')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Fruit tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Guava'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Holly, fosters'], 'Ilex attenuata')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Lageoemia indica'], 'Lagerstroemia indica')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Lageoemiasarahsfavorite'], 'Lageoemia indica')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Lagerstroemiasarahsfavorite'], 'Lagerstroemia indica')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Lagertsroemia indicafauriei'], 'Lagerstroemia indica')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['LigumÂ '], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Linden'], 'Tilia sp.')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Locust'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['London plane'], 'Platanus acerifolia')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Longleaf pine'], 'Pinus palis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Magnolialiliflora liliflora'], 'Magnolia liliiflora')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Malcura pomnifera'], 'Maclura pomifera')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['No replant'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['No tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Non-sufficient space'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Osage-maclura pomifera'], 'Maclura pomifera')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Other tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Palm (unknown'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Palm \\(unknown'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Palm evergreen'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Palm sp'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Palm'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Persimmon diospyros'], 'Diospyros')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Persimmon, japanese'], 'Diospyros kaki')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Pheonix robelenii'], 'Phoenix roebelenii')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Pheonix'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Philadelpus'], 'Philadelphus')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Pine tree'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Pistache chinensis'], 'Pistacia chinensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Planting site'], '')   \n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Planting space'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Platanusxacerifolia'], 'Platanus acerifolia')\n",
    "        # synonyms for the London Plane\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Platanus hispanica'], 'Platanus acerifolia')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Platanus hybrida'], 'Platanus acerifolia')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Poor planting'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Potential site'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Preakness pl'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Private shrub'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Privet'], 'Ligustrum')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Queen palm'], 'Syagrus romanzoffiana')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Quercia agrifolia'], 'Quercus agrifolia')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Quercia virginiana'], 'Quercus virginiana')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Raphiolepsis'], 'Rhaphiolepis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Reference location'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Resident refusal'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Rhodendron'], 'Rhododendron sp.')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Rose-of-hibiscus syriacus'], 'Hibiscus syriacus')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Secretariat dr'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Shrub shrub'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Shrub'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Stump -'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Stump'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Taxas sp'], 'Taxas')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Taxas'], 'Taxus')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Tbd'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Tibuchnia'], 'Tibouchina')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Tili'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Tree(s)'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Tree\\(s\\)'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Unidientified'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Unsuitable site'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant -'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant planting'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant site'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant site/inadequate'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant site/ok'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant site/prop.'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant well'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant/inadequate spacing'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant/ok to'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Vacant/prop. owner'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['W-no sewer/water'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['W-no sewer\\/water'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['W-obstruction at'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['W-property owner'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['W\\-no sewer/water'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['W\\-ouction at'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Walnut tree'], 'Juglans')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Xancotheras sorbifolium'], 'Xanthoceras sorbifolium')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['Yellow-callitropsis nootkatensis'], 'Callitropsis nootkatensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['\\)amelanchier canadensis'], 'Amelanchier canadensis')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['\\)carpinus caroliniana'], 'Carpinus caroliniana')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['\\_\\_planting request'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['__planting request'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['_do not'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['_multiple species'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['_multiple'], '')\n",
    "        data['scientific_name'] = data['scientific_name'].replace(['_planting space'], '') \n",
    "        ## find genus only rows\n",
    "        # data['genus_only']=\"full\"\n",
    "        # data['genus_only'][data['scientific_name'].str.contains(' sp.',na=False)] = \"genus_only\"\n",
    "#############################\n",
    "###  Common Name       \n",
    "#############################\n",
    "    if 'common_name' in data.columns:\n",
    "        data['common_name_old']=data['common_name']\n",
    "        ## remove non ASCII characters\n",
    "        data['common_name'] = data['common_name'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "        ## quick fix to capitalization issues\n",
    "        data['common_name']=data['common_name'].str.capitalize()\n",
    "        ## remove cultivar names Maple 'freeman variety'\n",
    "        data['common_name']=data['common_name'].str.replace(r'\\'.*?\\'', '')\n",
    "        ## get rid of leading spaces\n",
    "        data['common_name']=data['common_name'].str.strip()\n",
    "        ## remove SPP issues\n",
    "        data['common_name'] = data['common_name'].str.replace(\" spp.\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"spp. \", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"spp\", '')\n",
    "        ## Remove random meaningless information\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Other\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Columnar \", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Various\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Unknown\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Unknown 1\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Unknown other\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Unkns\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Unknt\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Bdl ot\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Bdm ot\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Bds ot\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\"Nrwy\", 'Norway')\n",
    "        ## fix Maple, Red and \"Maple: Red\"\n",
    "        data['common_name'] = data['common_name'].str.split(', ').str[::-1].str.join(' ')\n",
    "        data['common_name'] = data['common_name'].str.split(': ').str[::-1].str.join(' ')\n",
    "        data['common_name'] = data['common_name'].str.split('; ').str[::-1].str.join(' ')\n",
    "        data['common_name'] = data['common_name'].str.split('-').str[::-1].str.join(' ')\n",
    "        data['common_name'] = data['common_name'].str.split(';').str[::-1].str.join(' ')\n",
    "        ## remove random commas, dashes, and semicolons\n",
    "        data['common_name'] = data['common_name'].str.replace(\",\", '')\n",
    "        data['common_name'] = data['common_name'].str.replace(\";\", '')\n",
    "        ## redo capitalization\n",
    "        data['common_name']=data['common_name'].str.capitalize()\n",
    "        ## redo get rid of leading spaces\n",
    "        data['common_name']=data['common_name'].str.strip()\n",
    "#############################\n",
    "###  Address Issues       \n",
    "#############################\n",
    "    if 'address' not in data.columns:\n",
    "        if 'street_name' in data.columns:\n",
    "            if 'street_number' in data.columns:\n",
    "                data['address']=data['street_number'].astype(str).str.replace('\\.0', '') + \" \" + data[\"street_name\"].str.strip().str.title().str.replace('\\/', '')    \n",
    "            if 'street_number' not in data.columns:\n",
    "                data['address']=data[\"street_name\"].str.strip().str.title().str.replace('\\/', '')\n",
    "    if 'address' in data.columns:\n",
    "        data['address_old']=data['address']\n",
    "        data['address']=data['address'].str.title().str.replace('Th ', 'th ').str.replace('Rd ', 'rd ')\n",
    "        data['address']=data['address'].str.replace('^0+','', regex=True)\n",
    "        data['address']=data['address'].str.replace('^Nan ','', regex=True)\n",
    "        data['address']=data['address'].str.replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "#############################\n",
    "###  Overhead Utility       \n",
    "#############################\n",
    "    if 'overhead_utility' in data.columns:\n",
    "        data['overhead_utility_old'] = data['overhead_utility']\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('None', 'No')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('No Lines', 'No')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('No Overhead Utilities', 'No')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Both', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Electric', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Communication', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Present and Conflicting', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Present \\/ No Conflict', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Present', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Closer than 20 ft', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Conflicting', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Both', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Low Voltage', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('High Voltage', 'Yes')\n",
    "        data['overhead_utility'] = data['overhead_utility'].str.replace('Unknown','')\n",
    "#############################\n",
    "### columns that just need simple capitalization fixes, etc\n",
    "#############################\n",
    "    for my_column in ['location_type','location_name',\n",
    "                      'neighborhood','district','planter_type','ward']:\n",
    "        if my_column in data.columns:\n",
    "            ## save original formatting\n",
    "            data[my_column+'_old']=data[my_column]\n",
    "            if data[my_column].dtype !=\"float64\":\n",
    "                if data[my_column].dtype !=\"int64\":\n",
    "                    ## remove non ASCII characters\n",
    "                    data[my_column] = data[my_column].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "                    ## quick fix to capitalization issues\n",
    "                    data[my_column]=data[my_column].str.capitalize()\n",
    "                    ## remove leading and trailing spaces\n",
    "                    data[my_column]=data[my_column].str.strip()\n",
    "#############################\n",
    "### height convert FT to M\n",
    "#############################\n",
    "    if \"height_FEET\" in data.columns: \n",
    "        data['height_M'] = data['height_FEET']*3.28084\n",
    "#############################\n",
    "### standardize height range\n",
    "#############################\n",
    "    if \"height_range\" in data.columns:\n",
    "        data['height_range'] = data['height_range'].str.replace('15\\-30','15 to 30 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('\\-\\-\\-','')\n",
    "        data['height_range'] = data['height_range'].str.replace('01\\-15','0 to 15 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('30\\-45','30 to 45 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('45\\-60','45 to 60 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('60\\+','more than 60 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('LT 10\\'','0 to 10 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('11\\-20\\'','11 to 20 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('21\\-30\\'','21 to 30 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('31\\-40\\'','31 to 40 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('41\\-50\\'','41 to 50 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('GT 50\\'','more than 50 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('0\\-20','0 to 20 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('20\\-40','20 to 40 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('60\\-80','60 to 80 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('40\\-60','40 to 60 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('80\\+','more than 80 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('0ft\\-15ft','0 to 15 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('30ft\\-60ft','30 to 60 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('15ft\\-30ft','15 to 30 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('60ft\\+','more than 60 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('Over 25 ft','more than 25 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('15 \\- 25 ft','15 to 25 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('6 \\- 15 ft','6 to 15 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('Under 6 ft','less than 6 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('under 25\\'','less than 25 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('25\\' \\- 45\\'','25 to 45 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('over 45\\'','more than 45 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('20-50','20 to 50 ft')\n",
    "        data['height_range'] = data['height_range'].str.replace('50 \\+','more than 50 ft')\n",
    "        data['height_range']=data['height_range'].str.strip()\n",
    "#############################\n",
    "### bin heights\n",
    "#############################\n",
    "    if 'height_M' not in data.columns:\n",
    "        if 'height_range' in data.columns:\n",
    "            data['height_binned_M'] = data['height_range']\n",
    "            data['height_binned_M'] = data['height_binned_M'].replace(['0 to 15 ft'],'0 to 4.57 m')\n",
    "            data['height_binned_M'] = data['height_binned_M'].replace(['15 to 30 ft'], '4.57 to 9.14 m')\n",
    "            data['height_binned_M'] = data['height_binned_M'].replace(['30 to 45 ft'], '9.14 to 13.71 m')\n",
    "            data['height_binned_M'] = data['height_binned_M'].replace(['45 to 60 ft'], '13.71 to 18.29 m')\n",
    "            data['height_binned_M'] = data['height_binned_M'].replace(['more than 60 ft'], 'more than 18.29 m')\n",
    "    if 'height_M' in data.columns:    \n",
    "        data.loc[(data.height_M>0)&(data.height_M<=4.57),'height_binned_M']='0 to 4.57 m'\n",
    "        data.loc[(data.height_M>4.57)&(data.height_M<=9.14),'height_binned_M']='4.57 to 9.14 m'\n",
    "        data.loc[(data.height_M>9.14)&(data.height_M<=13.71),'height_binned_M']='9.14 to 13.71 m'\n",
    "        data.loc[(data.height_M>13.71)&(data.height_M<=18.29),'height_binned_M']='13.71 to 18.29 m'\n",
    "        data.loc[(data.height_M>18.29),'height_binned_M']='more than 18.29 m'\n",
    "    ## filter by the good bins\n",
    "    good_height_bins = ['0 to 4.57 m', '4.57 to 9.14 m', '9.14 to 13.71 m','13.71 to 18.29 m','more than 18.29 m']\n",
    "    if 'height_binned_M' in data.columns:  \n",
    "        data['height_binned_M']=data['height_binned_M'].str.strip()\n",
    "        data.loc[~(data.height_binned_M.isin(good_height_bins)),'height_binned_M']=''       \n",
    "#############################\n",
    "### diam breast height convert IN to CM\n",
    "#############################\n",
    "    if \"diameter_breast_height_IN\" in data.columns: \n",
    "        data['diameter_breast_height_CM'] = data['diameter_breast_height_IN']*2.54   \n",
    "#############################\n",
    "### diam breast height range\n",
    "#############################\n",
    "    if 'diameter_breast_height_range' in data.columns:\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('0-3in', '0 to 3 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('12-18in', '12 to 18 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('18-24in', '18 to 24 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('24-30in', '24 to 30 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('3-6in', '3 to 6 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('6-12in', '6 to 12 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('>30in', 'more than 30 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('0-6in', '0 to 6 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('0-15', '0 to 15 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('16-30', '16 to 30 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('31-45', '31 to 45 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('46-60', '46 to 60 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('>60', 'more than 60 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('12 - 24 in', '12 to 24 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('3 - 6 in', '3 to 6 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('6 - 12 in', '6 to 12 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('Over 24 in', 'more than 24 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].str.replace('Under 3 in', '0 to 3 in')\n",
    "        data['diameter_breast_height_range'] = data['diameter_breast_height_range'].replace(['6 to 12'], '6 to 12 in')\n",
    "#############################\n",
    "### diam breast height binned\n",
    "#############################\n",
    "    if 'diameter_breast_height_CM' not in data.columns:\n",
    "        if 'diameter_breast_height_range' in data.columns:\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_range']\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_binned'].replace(['7 to 12 in'], '6 to 12 in')\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_binned'].replace(['0 to 3 in','3 to 6 in'], '0 to 6 in')\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_binned'].replace(['13 to 18 in'], '12 to 18 in')\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_binned'].replace(['19 to 24 in'], '18 to 24 in')\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_binned'].replace(['25 to 30 in'], '24 to 30 in')\n",
    "            data['diameter_breast_height_binned'] = data['diameter_breast_height_binned'].replace(['31 to 36 in', '37 to 42 in', '43 to 48 in', 'more than 49 in', \n",
    "                                                                                                   '30 to 36 in', '36 to 42 in', '42 to 48 in','more than 48 in', \n",
    "                                                                                                   'more than 42 in', 'more than 36 in'], 'more than 30 in')\n",
    "    ## make a CM column\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned']\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned_CM'].replace(['0 to 6 in'], '0 to 15.24 cm')\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned_CM'].replace(['6 to 12 in'], '15.24 to 30.48 cm')\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned_CM'].replace(['12 to 18 in'], '30.48 to 45.72 cm')\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned_CM'].replace(['18 to 24 in'], '45.72 to 60.96 cm')\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned_CM'].replace(['24 to 30 in'], '60.96 to 76.2 cm')\n",
    "            data['diameter_breast_height_binned_CM'] = data['diameter_breast_height_binned_CM'].replace(['more than 30 in'], 'more than 76.2 cm')\n",
    "    if 'diameter_breast_height_CM' in data.columns:    \n",
    "        data.loc[(data.diameter_breast_height_CM>0)&(data.diameter_breast_height_CM<=15.24),'diameter_breast_height_binned_CM']='0 to 15.24 cm'\n",
    "        data.loc[(data.diameter_breast_height_CM>15.24)&(data.diameter_breast_height_CM<=30.48),'diameter_breast_height_binned_CM']='15.24 to 30.48 cm'\n",
    "        data.loc[(data.diameter_breast_height_CM>30.48)&(data.diameter_breast_height_CM<=45.72),'diameter_breast_height_binned_CM']='30.48 to 45.72 cm'\n",
    "        data.loc[(data.diameter_breast_height_CM>45.72)&(data.diameter_breast_height_CM<=60.96),'diameter_breast_height_binned_CM']='45.72 to 60.96 cm'\n",
    "        data.loc[(data.diameter_breast_height_CM>60.96)&(data.diameter_breast_height_CM<=76.2),'diameter_breast_height_binned_CM']='60.96 to 76.2 cm'\n",
    "        data.loc[(data.diameter_breast_height_CM>76.2),'diameter_breast_height_binned_CM']='more than 76.2 cm'\n",
    "    ## filter by the good bins\n",
    "    good_bins = ['0 to 15.24 cm', '15.24 to 30.48 cm', '30.48 to 45.72 cm', '45.72 to 60.96 cm', \n",
    "                 '60.96 to 76.2 cm', 'more than 76.2 cm']\n",
    "    if 'diameter_breast_height_binned_CM' in data.columns:  \n",
    "        data.loc[~(data.diameter_breast_height_binned_CM.isin(good_bins)),'diameter_breast_height_binned_CM']='' \n",
    "#############################\n",
    "### condition (including alive_status)\n",
    "#############################        \n",
    "    if \"alive_status\" in data.columns:\n",
    "        data['alive_status'] = data['alive_status'].replace(['Proposed Site - Large'], '')\n",
    "        data['alive_status'] = data['alive_status'].replace(['Recently Planted'], '')\n",
    "        data['alive_status'] = data['alive_status'].replace(['Removed'], '')\n",
    "        data['alive_status'] = data['alive_status'].replace(['Stump'], 'dead')\n",
    "        data['alive_status'] = data['alive_status'].replace(['False'], 'dead')\n",
    "        data['alive_status'] = data['alive_status'].replace(['True'], 'alive')\n",
    "        data['alive_status'] = data['alive_status'].replace(['Dead'], 'dead')\n",
    "        data['alive_status'] = data['alive_status'].replace(['Alive'], 'alive')\n",
    "    if \"condition\" in data.columns:\n",
    "        data['condition'] = data['condition'].str.capitalize()\n",
    "        data['condition'] = data['condition'].replace(['Dead', 'Dead 0%'], 'dead')\n",
    "        data['condition'] = data['condition'].replace(['Dead / dying', 'Dead/dying', 'Dead/nearly dead/planting space/stump', \n",
    "                                                       'Dead or dying'], 'dead/dying')\n",
    "        data['condition'] = data['condition'].replace(['Excellent', 'Very good', 'Excellent / specimen tree', \n",
    "                                                       'Excellent 100%', 'Very good 90%'], 'excellent')\n",
    "        data['condition'] = data['condition'].replace(['Fair - minor problems', 'Fair', 'Fair / generally healthy', \n",
    "                                                       'Fair 60%'], 'fair')\n",
    "        data['condition'] = data['condition'].replace(['Good - no apparent problems', 'Good', 'Healthy', \n",
    "                                                       'Good / healthy and vigorous', 'Good 80%'], 'good')\n",
    "        data['condition'] = data['condition'].replace(['','0','2','3','4','Not Applicable','Not applicable', \n",
    "                                                       'Cannot be determined', 'Stump', 'Removal', 'Unknown',\n",
    "                                                       'Not entered', 'Remove', 'Field chk', 'Not check*', \n",
    "                                                       'Vacant', 'Unassigned'], 'NA')\n",
    "        data['condition'] = data['condition'].replace(['Poor - major problems', 'Poor', 'Poor ','Very Poor','Very poor', \n",
    "                                                       'Critical', 'Dying', 'Stressed', 'Poor / declining', \n",
    "                                                       'Condition 20%', 'Critical 20%', 'Poor 40%', 'Very poor 30%'], 'poor')\n",
    "\n",
    "#############################\n",
    "### date columns\n",
    "#############################\n",
    "# cycle through all date-like columns\n",
    "# get them all into 10/02/1990 format\n",
    "    for my_date_column in ['planted_date', 'inventory_date', 'condition_date',\n",
    "                       'edit_date', 'inspected_date', 'retired_date']:\n",
    "        if my_date_column in data.columns:\n",
    "            data[my_date_column] = data[my_date_column].replace([0,-1,99], '')\n",
    "            data[my_date_column] = data[my_date_column].astype(str)\n",
    "            # remove time from columns that have it, delimited by a T or space\n",
    "            data.loc[data[my_date_column].str.contains('T'), my_date_column] = data[my_date_column].str.split('T').str[0]\n",
    "            data.loc[data[my_date_column].str.contains(' '), my_date_column] = data[my_date_column].str.split(' ').str[0]\n",
    "            # change 2018.0 to 2018\n",
    "            data.loc[data[my_date_column].str.contains('.'), my_date_column] = data[my_date_column].str.split('.').str[0]\n",
    "            # reformat 1990-12-2 to 12/02/1990. It is year-month-day format for the cities\n",
    "            dates_with_dashes = data.loc[data[my_date_column].str.contains('-'), my_date_column]\n",
    "            year = dates_with_dashes.str.split('-').str[0]\n",
    "            month = dates_with_dashes.str.split('-').str[1]\n",
    "            day = dates_with_dashes.str.split('-').str[2]\n",
    "            new_dates_with_dashes = month + '/' + day +'/'+year\n",
    "            data.loc[data[my_date_column].str.contains('-'), my_date_column]=new_dates_with_dashes\n",
    "            # sometimes they format it 2020/09/15. We need to find those cases and reorder.\n",
    "            dates_with_slashes = data.loc[data[my_date_column].str.contains('/'), my_date_column]\n",
    "            first = dates_with_slashes.str.split('/').str[0]\n",
    "            second = dates_with_slashes.str.split('/').str[1]\n",
    "            third = dates_with_slashes.str.split('/').str[2]\n",
    "            # is the first value the year?\n",
    "            count_yearlike = sum(i > 1900 for i in first.value_counts().sort_values(ascending=False).keys().astype(int)[0:10]) \n",
    "            # set threshold at 8 out of 10 need to be yearlike, assuming some blanks etc\n",
    "            if count_yearlike >= 8:\n",
    "                new_dates_with_slashes = second + '/' + third +'/'+ first\n",
    "                data.loc[data[my_date_column].str.contains('/'), my_date_column]=new_dates_with_slashes              \n",
    "# now find the most recent \"observation\" out of inventory, condition, edit, and inspected\n",
    "    observation_date_columns = ['inventory_date', 'condition_date','edit_date', 'inspected_date']\n",
    "    observations = data.loc[:,data.columns.isin(observation_date_columns)]\n",
    "    number_columns = observations.shape[1]\n",
    "    if number_columns == 1:\n",
    "        data['most_recent_observation'] = observations\n",
    "        data['most_recent_observation_type'] = observations.columns[0]\n",
    "    if number_columns > 1:\n",
    "        for c in observations.columns:\n",
    "            observations[c] = pd.to_datetime(observations[c],errors = 'coerce')\n",
    "        data['most_recent_observation'] = observations.max(axis=1)\n",
    "        data['most_recent_observation'] =  data['most_recent_observation'].astype(str)\n",
    "        ## pd.to_datetime automatically formats it as 2020-03-30, so I change it back here\n",
    "        observations_with_dashes = data.loc[data['most_recent_observation'].str.contains('-'), 'most_recent_observation']\n",
    "        year_o = observations_with_dashes.str.split('-').str[0]\n",
    "        month_o = observations_with_dashes.str.split('-').str[1]\n",
    "        day_o = observations_with_dashes.str.split('-').str[2]\n",
    "        new_observations_with_dashes = month_o + '/' + day_o +'/'+year_o\n",
    "        data.loc[data['most_recent_observation'].str.contains('-'), 'most_recent_observation']=new_observations_with_dashes\n",
    "#############################\n",
    "### location_type\n",
    "#############################            \n",
    "    if \"location_type\" in data.columns:\n",
    "        data['location_type_old'] = data['location_type']\n",
    "        data['location_type'] = data['location_type'].replace(['999'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['\\<null\\>'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['1st row'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['2nd row'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['3rd row'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['4th row'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Across the street'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Arboretum'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Arterial roadway'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Arts commission'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Asian arts commission'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Athletic field'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Back of sidewalk'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Backup'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Baseball field'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Basin'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Basketball court'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Building'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Business/commercial'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Cart path'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Cemetery'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Church / school'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['City building'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['City college'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['City maintained street tree'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['City owned parcel'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['City property'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Cjots'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Cns'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Cnty'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Contrctr'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Curbtight'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Cutout'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Denver fire dept'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Dept of real estate'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Disc golf course'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Dog park'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Don'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Downtown'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Dpw'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Dpw for city agency'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Driving range'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Easement tree'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Entrance'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Facility'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Fairway'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Fence row'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Ffd'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Fire dept'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Golf course'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Golf course rough'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Health dept'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Hlcc'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Hoa street tree'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Housing authority'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Industrial/ large commercial'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Industrial/large commercial'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Institutional'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Island'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Lawn'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Maintenance facilit'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Mayor office of housing'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Median'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Mountain park'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Mta'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Multi family'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Multi-family residential'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Natural area'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Ncns'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Not entered'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Nursery'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Office of mayor'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Offsetfromcurb'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Oncurb'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Open'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Open/unrestricted'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Orphan parcel'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Other'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Other public property'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Other un-maintained locations'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Other/maintained'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Park'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Park strip'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Park/ vacant/ other'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Park/golf course'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Park/greenbelt'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Park/public space'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Park/vacant/other'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Parking area'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Parkway'], '')\n",
    "        data['location_type'] = data['location_type'].replace(['Paseo'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Paver'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Picnic area'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Pinetum'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Planter'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Playground'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Plaza'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Police dept'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Pond or creek'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Port'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Pos'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Priv'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Private'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Private maintained street tree'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Private property tree'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Public library'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Puc'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Pump house'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Purchasing dept'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Raised planter'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Rec/park'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Residential private tree'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Residential/commercial'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Right-of-way'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Row 1 ats'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Row1 across street'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Row2 across street'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Row3 across street'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['School'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['School site'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Scl'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Sdot'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Seac'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Sfusd'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Sidewalk'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Single family'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Single-family residential'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Small commercial'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Sndtr'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Sph'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Spray pool'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Sps'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Spu'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Stewardship tree'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Street'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Street tree'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Street tree (boulevard or median)'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Streetscape'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Strip'], 'built_environment')\n",
    "        data['location_type'] = data['location_type'].replace(['Swale'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Swimming pool'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Tennis court'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Traffic island'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Trail'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Tree lawn'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['Tree well'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Unassigned'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Unmaintained area'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Vacant/other'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Walking path'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace(['War memorial'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Well or pit'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Well/pit'], 'no_info')\n",
    "        data['location_type'] = data['location_type'].replace(['Woodline'], 'green_space')\n",
    "        data['location_type'] = data['location_type'].replace([''], 'no_info')           \n",
    "#############################\n",
    "#############################\n",
    "#############################\n",
    "###\n",
    "### write CSV\n",
    "###\n",
    "#############################\n",
    "#############################\n",
    "#############################\n",
    "    data.to_csv(path + '\\\\'+ state + '_edited.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Worcester_trees.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if error see which file is the issue\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CSV of all scientific names\n",
    "To check in R taxize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning\\Sheets_Data_Misspellings_Corrected\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning\\Sheets_Data_Misspellings_Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_edited = glob.glob('*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get value counts for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (15,18,25,32,38,39,40,41,42,45,50,62,63) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (24,25,26,27,28,29,30,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (16,17,18,19,20,24,25,26,27,28,29,30,31,32,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (6,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (18,20,48) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (0,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4,5,9,11,12,17,18,34,37,39,57,67,70) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (34,35,36,37,40,45,71,72) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (37,38,71,72) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (71) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4,14,15,17,19,21,40,51,62,71,72,73,75,83) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (3,31,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (11,12,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (7,9,10,21,39) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (9,35,45) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (23,27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\dakot\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (26,27,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# # set up a \"dictionary\" for our results\n",
    "results = {}\n",
    "for file in csv_files_edited:\n",
    "   # read the file\n",
    "   data = pd.read_csv(file)\n",
    "   # save the state name\n",
    "   state = file.split('_')[0]\n",
    "   #if it has scientific names\n",
    "   if 'scientific_name' in data.columns:\n",
    "        list=data.groupby(['scientific_name']).size().reset_index().rename(columns={0:'count'})\n",
    "        results[file] = list\n",
    "        \n",
    "# concatenate everything and get good labels\n",
    "all_scientific=pd.concat(results,axis=0)\n",
    "species = all_scientific.reset_index()\n",
    "species = species.drop(['level_1'],axis=1)\n",
    "species=species.rename(columns={'level_0':'file'})\n",
    "\n",
    "# print to csv\n",
    "path=r'C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning'\n",
    "species.to_csv(path +'\\\\' + 'Species_By_City.csv')\n",
    "\n",
    "## to get total count\n",
    "species_count = pd.DataFrame(species.groupby(['scientific_name'])['count'].sum().sort_values()).reset_index()\n",
    "species_count.to_csv(path +'\\\\' + 'Species_Counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CSV of common names\n",
    "to send to taxize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up a \"dictionary\" for our results\n",
    "results = {}\n",
    "for file in csv_files_edited:\n",
    "   # read the file\n",
    "   data = pd.read_csv(file)\n",
    "   #if it has common names but no scientific names\n",
    "   if \"scientific_name\" not in data.columns:\n",
    "        if 'common_name' in data.columns:\n",
    "            list=data.groupby(['common_name']).size().reset_index().rename(columns={0:'count'})\n",
    "            results[file] = list\n",
    "        \n",
    "# concatenate everything and get good labels\n",
    "all_scientific=pd.concat(results,axis=0)\n",
    "species = all_scientific.reset_index()\n",
    "species = species.drop(['level_1'],axis=1)\n",
    "species=species.rename(columns={'level_0':'city'})\n",
    "\n",
    "# print to csv\n",
    "path=r'C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning'\n",
    "species.to_csv(path +'\\\\' + 'Common_Names_By_City.csv')\n",
    "\n",
    "## to get total count\n",
    "species_count = pd.DataFrame(species.groupby(['common_name'])['count'].sum().sort_values()).reset_index()\n",
    "species_count.to_csv(path +'\\\\' + 'Common_Names_Counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get CSV of all city and state names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for file in csv_files_edited:\n",
    "   # read the file\n",
    "   data = pd.read_csv(file)\n",
    "   # save the city name\n",
    "   city = file.split('_')[0]\n",
    "   #if it has common names but no scientific names\n",
    "   list=data.groupby(['city','state']).size().reset_index().rename(columns={0:'count'})\n",
    "   results[file] = list\n",
    "            \n",
    "# concatenate everything and get good labels\n",
    "all_cities=pd.concat(results,axis=0).reset_index().drop(['level_1'],axis=1).rename(columns={'level_0':'filename'})\n",
    "# print to csv\n",
    "path=r'C:\\Users\\dakot\\Documents\\Trees\\Data Cleaning'\n",
    "all_cities.to_csv(path +'\\\\' + 'Cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Cleaning_CityTrees.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
